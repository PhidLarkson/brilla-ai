{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSMQ - Kwame AI Project\n",
    "\n",
    "###### Title: A script For Parsing HTML (Using URLs) into Sections and Paragraphs.\n",
    "###### By: Ernest Samuel, Team member; Data preprocessing Team\n",
    "###### Date: 24-06-2023\n",
    "\n",
    "# Data processing functions\n",
    "This script contains Five (5) functions:\n",
    "\n",
    "1. unique(array): For removing all duplicates from processed data. It accepts a list of items and removes duplicates from it\n",
    "\n",
    "2. extract_rawTable_of_content(link, homePage): it takes the URL of the targeted website, up until the last forward slash \"/\" which is referred to as 'link', and the strings of characters after the forward slash \"/\" these characters are expected to the specific for the home page and is referred as 'homePage'. Then extract the table of contents of the textbook\n",
    "\n",
    "3. extract_url(link, pageList, maxNmber, char): It extracts from the list of the table of content and concatenate it with the link to form the desired page URL\n",
    "\n",
    "*         link: just as described in the function above.\n",
    "\n",
    "*         pageList: A list containing the table of contents extracted from the previous function\n",
    "        \n",
    "*         maxNmber:  The maximum index number on the table of content of the desired textbook\n",
    "\n",
    "*         char(Optional): A list containing the first character of the index in the table of content that is not a number\n",
    "\n",
    "4. extract_url_content(url, file_name): Takes the textbook's URL and name of the textbook, extracts and structures the data from the URL, and stores it with the file name. NB: file_name is optional.\n",
    "\n",
    "5. extract_textbook(url_list, textbook_name): Iterates over the list of URL list, generated by function 3, parse it into function 4 to extract the contents, and then save it as a JSON file with the name of the textbook 'textbook_name'.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract URL for each pages of the Textbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------Function: Remove dublicates-------------------------#\n",
    "\n",
    "def unique(array):\n",
    "# This funtion takes a list and remove dublicates\n",
    "    return list(dict.fromkeys(array))\n",
    "\n",
    "\n",
    "def remove_items(a, b):\n",
    "    # this function removes a subset from a supperset of a list.\n",
    "    # it was used to filter out Paaragraph_Not_in_Section\n",
    "    # a = supperset = referring to all paragraphs (Para) , b = subset = referring to all sectioned paragraphs\n",
    "    a = [item for item in a if item not in b]\n",
    "    return a\n",
    "\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "#-----------Function: Extract table of contents -----------------#\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "def extract_rawTable_of_content(link, homePage):\n",
    "\n",
    "    # --> homePage = First landing pages of online view of the textbook(eg: \"1-introduction\")\n",
    "    # ---> link = url of the site excluding landing page indexing (eg: \"https://openstax.org/books/university-physics-volume-3/pages/\")\n",
    "\n",
    "    website_link = link+homePage\n",
    "    url_list = []\n",
    "    \n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(website_link)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the table of contents div using its class\n",
    "        table_of_contents_div = soup.find('div')\n",
    "        \n",
    "        if table_of_contents_div:\n",
    "            # Find all the <a> tags within the table of contents div\n",
    "            a_tags = table_of_contents_div.find_all('a')\n",
    "            \n",
    "            # Extract the href attribute from each <a> tag and store it in the list\n",
    "            for a_tag in a_tags:\n",
    "                href = a_tag.get('href')\n",
    "                url_list.append(href)\n",
    "\n",
    "\n",
    "                # if href.startswith(\"h\"):\n",
    "            \n",
    "        else:\n",
    "            print(\"Table of contents div not found on the website.\")\n",
    "                \n",
    "    return unique(url_list) \n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "#----------- Extract URL for each page of the textbook -----------#\n",
    "\n",
    "def extract_url(link, pageList, maxNmber, char = []):\n",
    "    # This function generates the dirst number or alphabet that\n",
    "    # is part of the table of content requied\n",
    "    #------------------------------------------------------------------------#\n",
    "\n",
    "    #-> maxNumber = maximum number of the index numbers of the table of content\n",
    "    #--> char =  alphabets or string index in the table of content\n",
    "    # --> pageList = a list containing landing pages of all needed charpter(eg: [\"1-introduction\", 'chapter-2' .. ])\n",
    "    # ---> link = url of the site excluding landing page indexing (eg: \"https://openstax.org/books/university-physics-volume-3/pages/\")\n",
    "\n",
    "    #------------------------------------------------------------------------#\n",
    "    url = []\n",
    "    list_pages = list(range(1,maxNmber+1))\n",
    "    for item in range(len(char)):\n",
    "        list_pages.append(char[item])\n",
    "\n",
    "    for item in pageList:\n",
    "        for value in list_pages:\n",
    "        \n",
    "            if item.startswith(str(value)):\n",
    "                url.append(link+item)\n",
    "\n",
    "    return  unique(url)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract contents from a URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_url_content(url, file_name=''):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the main content section\n",
    "    main_content = soup.find(\"div\")\n",
    "\n",
    "   \n",
    "    if main_content is None:\n",
    "        print(\"Unable to find the main content section\")\n",
    "        return\n",
    "\n",
    "    # Find the first title on the website\n",
    "    titles = main_content.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\"])\n",
    "    first_title = None\n",
    "    for title in titles:\n",
    "        if title.text.strip():\n",
    "            first_title = title.text.strip()\n",
    "            first_title = str(file_name)+'-'+ first_title\n",
    "            break\n",
    "    file_name = first_title if first_title else str(file_name)+'-'+\"content\"\n",
    "\n",
    "    # List to store the content\n",
    "    content_list = []\n",
    "\n",
    "     #----------------------------\n",
    "     # Title of the chapter\n",
    "    Heading = {}\n",
    "    head = soup.find('head')\n",
    "    Title = head.find_all('title')\n",
    "    \n",
    "\n",
    "    for head in Title:\n",
    "        if head:\n",
    "            name = head.text.strip()\n",
    "            Heading[\"Title\"] = name\n",
    "\n",
    "\n",
    "    body = soup.find('body')\n",
    "    paras =[]\n",
    "    pp = body.find_all('p')\n",
    "    \n",
    "    for p in pp:\n",
    "        paras.append(p.text.strip())\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "    # Find all the sections in the main content\n",
    "    sections = main_content.find_all(\"section\")\n",
    "\n",
    "    # Set to store unique section identifiers\n",
    "    section_identifiers = set()\n",
    "\n",
    "    # Iterate over each section\n",
    "    sec = [] # to generate sublicate of paragraph data\n",
    "    for section in sections:\n",
    "        section_data = {}\n",
    "\n",
    "        # Extract section identifier\n",
    "        section_id = section.get(\"id\")\n",
    "        section_class = section.get(\"class\")\n",
    "        section_uuid_key = section.get(\"data-uuid-key\")\n",
    "        section_data_type = section.get(\"data-type\")\n",
    "        section_class_tuple = tuple(section_class) if section_class is not None else ()\n",
    "        section_identifier = (section_id, section_class_tuple, section_uuid_key, section_data_type)\n",
    "\n",
    "        # Skip if section identifier is already encountered\n",
    "        if section_identifier in section_identifiers:\n",
    "            continue\n",
    "\n",
    "        # Add section identifier to the set\n",
    "        section_identifiers.add(section_identifier)\n",
    "\n",
    "        # Extract section title\n",
    "        #------------------------\n",
    "        subtitle = soup.find(['h3','h4','h2','h1'])\n",
    "        #subtitle = soup.find('h3')\n",
    "        #----------------\n",
    "        title = section.find([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\"])\n",
    "        if title:\n",
    "            section_data[\"title\"] = title.text.strip()\n",
    "        else:\n",
    "             section_data[\"title\"] = subtitle.text.strip()\n",
    "\n",
    "        # Extract section paragraphs\n",
    "        paragraphs = section.find_all([\"p\", \"span\"])\n",
    "        section_data[\"Section\"] = []\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_text = paragraph.text.strip()\n",
    "            if paragraph_text:\n",
    "                section_data[\"Section\"].append(paragraph_text)\n",
    "                sec.append(paragraph_text)\n",
    "\n",
    "\n",
    "        # Extract list items\n",
    "        lists = section.find_all(\"ul\")\n",
    "        section_data[\"lists\"] = []\n",
    "        for ul in lists:\n",
    "            list_items = ul.find_all(\"li\")\n",
    "            section_data[\"lists\"].append([li.text.strip() for li in list_items])\n",
    "\n",
    "        # Extract figures\n",
    "        figures = section.find_all(\"div\", {\"class\": \"os-figure\"})\n",
    "        section_data[\"figures\"] = []\n",
    "        for figure in figures:\n",
    "            figure_data = {}\n",
    "            img = figure.find(\"img\")\n",
    "            # if img:\n",
    "            #     image_url = urljoin(url, img[\"src\"])\n",
    "            #     figure_data[\"image\"] = image_url\n",
    "            # section_data[\"figures\"].append(figure_data)\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "            if img and \"src\" in img.attrs:\n",
    "                image_url = urljoin(url, img[\"src\"])\n",
    "                figure_data[\"image\"] = image_url\n",
    "\n",
    "            caption = figure.find(\"figcaption\")\n",
    "            if caption:\n",
    "                figure_data[\"caption\"] = caption.text\n",
    "            \n",
    "            section_data[\"figures\"].append(figure_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "        # Extract tables\n",
    "        tables = section.find_all(\"table\")\n",
    "        section_data[\"tables\"] = []\n",
    "        for table in tables:\n",
    "            table_data = []\n",
    "            rows = table.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                table_data.append([cell.text.strip() for cell in cells])\n",
    "            section_data[\"tables\"].append(table_data)\n",
    "\n",
    "        content_list.append(section_data)\n",
    "\n",
    "    # Extract only paragraphs that are not in sections structure\n",
    "    Heading[\"Paragraphs_Not_in_Sections\"]= remove_items(paras,sec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [Heading, content_list] # Heading returns the title of the chapter and every other text that are not in sections of the data structure\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract for all URL in the textbook and save as one file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_textbook(url_list, textbook_name):\n",
    "\n",
    "    # List to store the content\n",
    "    content_list = []\n",
    "    page_data = {}\n",
    "    pages = 0\n",
    "    for url in url_list:\n",
    "        page_content=extract_url_content(url)  # This is a list of two items: Heading = paragraphs_not_in_section \n",
    "                                                 # and content_list = the well structured sections that are needed.\n",
    "                                                 # we can just index it to remove paragraphs that are not in section.\n",
    "    \n",
    "        page_data['Page '+str(pages)] = page_content[1]\n",
    "        pages +=1\n",
    "        \n",
    "    content_list.append(page_data)\n",
    "\n",
    "    # Get the current working directory and create the file path for JSON\n",
    "    script_dir = os.getcwd()\n",
    "    json_path = os.path.join(script_dir, f\"{textbook_name}.json\")\n",
    "\n",
    "    # Save the content as JSON\n",
    "    with open(json_path, \"w\") as file:\n",
    "        json.dump(content_list, file, indent=4)\n",
    "\n",
    "    \n",
    "    return content_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biology 2e\n",
      "Algebra Trigonometry 2e\n",
      "Chemistry 2e\n",
      "College Physics 2e\n",
      "College Algebra 2e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read CSV file containing link to the text book\n",
    "import pandas as pd\n",
    "openStax = pd.read_csv('OpenStax Textbooks - Sheet2.csv')\n",
    "\n",
    "# select which textbook to extract\n",
    "openStax = openStax.iloc[[24,19,22,1,11]]\n",
    "\n",
    "# for each books, extract the URL content.\n",
    "for BookName, urls in zip(openStax['BOOKS'], openStax['URL']):\n",
    "   \n",
    "    print(BookName)\n",
    "    # initialize first page of view oline\n",
    "    landpage = 'preface'\n",
    "    site= str(urls)\n",
    "\n",
    "    # Extract table of content\n",
    "    pageList=extract_rawTable_of_content(site,landpage)\n",
    "\n",
    "    # Initialize first index of your requred contents from the table of contents\n",
    "    maxNmber = 10 # maximum number of numerical index on your table of value\n",
    "    char = ['a', 'b','c', 'd', 'e', 'f', 'g', 'i']\n",
    "\n",
    "\n",
    "    # Extract the list of URL to be parsed for website scraping (this is used in the htmlProcessing )\n",
    "    URLs = extract_url(site, pageList, maxNmber, char)\n",
    "\n",
    "    # Name of the textbook\n",
    "    textbook = str(BookName)\n",
    "    \n",
    "    # extract textbook\n",
    "    cont = extract_textbook(URLs, textbook)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Structure of the stored data\n",
    "\n",
    "- The data is stored page by page according to the website table of content in dictionary format.\n",
    "\n",
    "- For each page, the data is structured to store; titles and Sections in each title. List, Figures and Table for each Section.\n",
    "\n",
    "- Each section contains a list of paragraphs from the textbook.\n",
    "\n",
    "- Figure contains a list of links to an image if the image is identified in a section.\n",
    "\n",
    "- table contains all tabular data identified in the section.\n",
    "\n",
    "- List contains all ordered or unordered lists found in a section.\n",
    "\n",
    "## Observations\n",
    "\n",
    "- If the data is stored in JSON format, some special characters, including maths equations, are returned in an encoded format. \n",
    "\n",
    "- But, if we print the content without storing it in JSON format, the data is returned exactly as it is in the textbook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
